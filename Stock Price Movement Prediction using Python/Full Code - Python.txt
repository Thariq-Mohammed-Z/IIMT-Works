# Core Libraries
import numpy as np
import pandas as pd
import random
import math
import math as mt

# Visualization Libraries
import matplotlib.pyplot as plt
import seaborn as sns

# Machine Learning Libraries
from sklearn import preprocessing
from sklearn import linear_model
from sklearn import tree
from sklearn.linear_model import LogisticRegression
from sklearn.ensemble import RandomForestClassifier
from sklearn.neural_network import MLPClassifier
from sklearn.cluster import KMeans
from sklearn.tree import DecisionTreeClassifier
from sklearn.impute import SimpleImputer
from sklearn.preprocessing import StandardScaler
from sklearn.model_selection import train_test_split
from sklearn.model_selection import cross_val_score, GridSearchCV
from sklearn.model_selection import train_test_split
from imblearn.over_sampling import SMOTE
from sklearn.preprocessing import LabelEncoder
from sklearn.tree import plot_tree
import matplotlib.pyplot as plt
import seaborn as sns

# Metrics and Evaluation
from sklearn.metrics import confusion_matrix, classification_report
from sklearn.metrics import accuracy_score, roc_curve, auc, ConfusionMatrixDisplay, RocCurveDisplay, precision_score, recall_score, roc_auc_score, f1_score
from sklearn.metrics import silhouette_score, silhouette_samples

# Statistical Modeling
import statsmodels.api as sm
import statsmodels.formula.api as smf

# Association Rule Mining
from mlxtend.frequent_patterns import apriori
from mlxtend.frequent_patterns import association_rules
from mlxtend.preprocessing import TransactionEncoder

# Other Utilities
import re
import warnings
from math import exp
from math import log as LOG

# Load the first dataset
data1 = pd.read_csv('../Data/application_record.csv')

# Preview the first 5 rows of the dataset
print("First 5 rows of Dataset 1:")
print(data1.head())

# Load the second dataset
data2 = pd.read_csv('../Data/credit_record.csv')

# Preview the first 5 rows of the second dataset
print("\nFirst 5 rows of Dataset 2:")
print(data2.head())

# Merge the two datasets on the common column
merged_data = pd.merge(data1, data2, on='ID')

# Preview the first 5 rows of the merged dataset
print("\nFirst 5 rows of the merged dataset:")
print(merged_data.head())

# Check the structure and details of the merged dataset
print("\nMerged Dataset Information:")
print(merged_data.info())

# Save the merged dataset as a new CSV file
merged_data.to_csv('../Data/merged_dataset.csv', index=False)
print("\nMerged dataset saved as '../Data/merged_dataset.csv'")

# Preview the first few rows of the dataset
print("First 5 rows of the merged dataset:")
print(merged_data.head())

# Check the dataset's structure and types of columns
print("\nDataset Information:")
print(merged_data.info())

# Get summary statistics for numerical columns
print("\nSummary Statistics:")
print(merged_data.describe())

# Select numerical columns
numerical_cols = merged_data.select_dtypes(include=['int64', 'float64']).columns

# Plot histograms for numerical features
merged_data[numerical_cols].hist(figsize=(15, 10), bins=20, color='skyblue', edgecolor='black')
plt.suptitle("Distribution of Numerical Features", fontsize=16)
plt.show()

# Calculate correlations for numerical columns
numerical_cols = merged_data.select_dtypes(include=['int64', 'float64']).columns
correlation_matrix = merged_data[numerical_cols].corr()

# Plot the heatmap
plt.figure(figsize=(12, 8))
sns.heatmap(correlation_matrix, annot=True, cmap='coolwarm', fmt='.2f', linewidths=0.5)
plt.title("Correlation Heatmap of Numerical Features", fontsize=16)
plt.show()



# Select numerical columns
numerical_cols = merged_data.select_dtypes(include=['int64', 'float64']).columns

# Plot the pairplot
sns.pairplot(merged_data[numerical_cols], diag_kind='kde', corner=True)
plt.suptitle("Pairplot of Numerical Features", y=1.02, fontsize=16)
plt.show()

# Check for missing values in the dataset
print("\nMissing Values in Each Column:")
print(merged_data.isnull().sum())

# Visualize missing values in merged_data
plt.figure(figsize=(10, 6))
sns.heatmap(merged_data.isnull(), cbar=False, cmap='viridis')
plt.title("Missing Values Heatmap")
plt.show()

# Dropping the column 'OCCUPATION_TYPE'
merged_data = merged_data.drop(columns=['OCCUPATION_TYPE'])
print("Dropped 'OCCUPATION_TYPE' column.")

# Check for missing values in the dataset
print("\nMissing Values in Each Column:")
print(merged_data.isnull().sum())

# Check unique values for each column
print("\nUnique Values in Each Column:")
for column in merged_data.columns:
    print(f"{column}: {merged_data[column].nunique()} unique values")

# Check for duplicates in the ID column
if merged_data['ID'].nunique() == len(merged_data):
    print("No duplicates found in the 'ID' column. All IDs are unique.")
else:
    print(f"Duplicates found in the 'ID' column. There are {len(merged_data) - merged_data['ID'].nunique()} duplicate IDs.")

# Drop duplicate rows based on the 'ID' column
merged_data = merged_data.drop_duplicates(subset='ID')

# Verify after dropping duplicates
print("Duplicates removed. Current dataset shape:", merged_data.shape)

# Dropping irrelevant columns
merged_data = merged_data.drop(columns=['ID', 'FLAG_MOBIL'], errors='ignore')
print("Dropped 'ID' and 'FLAG_MOBIL' columns.")

# Convert DAYS_BIRTH to age in years
merged_data['AGE'] = (-merged_data['DAYS_BIRTH']) // 365

# Drop the original DAYS_BIRTH column as it is no longer needed
merged_data = merged_data.drop(columns=['DAYS_BIRTH'])
print("Converted 'DAYS_BIRTH' to 'AGE' and dropped the original column.")

# Preview the first few rows of the dataset
print("First 5 rows of the merged dataset:")
print(merged_data.head())

# Replace positive values in DAYS_EMPLOYED with 0 (indicating unemployment)
merged_data['DAYS_EMPLOYED'] = merged_data['DAYS_EMPLOYED'].apply(lambda x: x if x < 0 else 0)

# Convert DAYS_EMPLOYED to years (negative days to positive years)
merged_data['YEARS_EMPLOYED'] = -merged_data['DAYS_EMPLOYED'] // 365

# Drop the original column
merged_data = merged_data.drop(columns=['DAYS_EMPLOYED'])
print("Simplified: Transformed 'DAYS_EMPLOYED' to 'YEARS_EMPLOYED' and dropped the original column.")

# Preview the first few rows of the dataset
print("First 5 rows of the merged dataset:")
print(merged_data.head())

# Simplify STATUS to 'DEFAULT' (1 = Defaulted, 0 = Not Defaulted)
# Defaulted (1): Serious overdue or bad debts ('2', '3', '4', '5')
# Not Defaulted (0): Minor delays, paid off, or no loan ('0', '1', 'C', 'X')
merged_data['DEFAULT'] = merged_data['STATUS'].apply(
    lambda x: 1 if x in ['2', '3', '4', '5'] else 0
)

# Drop the original STATUS column as it's no longer needed
merged_data = merged_data.drop(columns=['STATUS'])

# Print confirmation
print("Simplified 'STATUS' into 'DEFAULT' (1 = Defaulted, 0 = Not Defaulted) and dropped the original column.")

# Initialize a dictionary to store label encoders
label_encoders = {}

# Encode all object (categorical) columns in the dataset
for column in merged_data.select_dtypes(include=['object']).columns:
    le = LabelEncoder()
    merged_data[column] = le.fit_transform(merged_data[column])
    label_encoders[column] = le  # Save the encoder for potential inverse transformation

print("Encoded all categorical variables using LabelEncoder.")

# Preview the first few rows of the dataset
print("First 5 rows of the merged dataset:")
print(merged_data.head())

# Initialize the scaler
scaler = StandardScaler()

# Scale the features (excluding the target column 'DEFAULT')
X = merged_data.drop(columns=['DEFAULT'])  # Features
y = merged_data['DEFAULT']  # Target

# Apply scaling
X_scaled = scaler.fit_transform(X)

print("Scaling applied to the dataset.")

# Define features (X) and target (y)
X = merged_data.drop(columns=['DEFAULT'])  # All columns except the target
y = merged_data['DEFAULT']  # Target column
print("Features (X) and Target (y) defined successfully.")

# Splitting the scaled dataset
X_train, X_test, y_train, y_test = train_test_split(
    X_scaled, y, test_size=0.2, random_state=42, stratify=y
)

print(f"Training set size: {X_train.shape}")
print(f"Test set size: {X_test.shape}")

# Applying SMOTE to oversample the minority class in the training set
smote = SMOTE(random_state=42)
X_train_resampled, y_train_resampled = smote.fit_resample(X_train, y_train)

# Print shapes to verify resampling
print("Training features shape after SMOTE:", X_train_resampled.shape)
print("Training target shape after SMOTE:", y_train_resampled.shape)

# Verify train-test split sizes
print(f"Original training set size: {X_train.shape}")
print(f"Original test set size: {X_test.shape}")

# Verify the number of features and samples after SMOTE
print(f"Training features shape after SMOTE: {X_train_resampled.shape}")
print(f"Training target shape after SMOTE: {y_train_resampled.shape}")

# Check class distribution in the original training set
print("Class distribution in original training set (before SMOTE):")
print(y_train.value_counts())

# Check class distribution in the resampled training set
print("\nClass distribution in resampled training set (after SMOTE):")
print(y_train_resampled.value_counts())

param_grids = {
    'Logistic Regression': {
        'model': LogisticRegression(max_iter=200),
        'params': {'C': [0.1, 1], 'solver': ['liblinear']}
    },
    'Decision Tree': {
        'model': DecisionTreeClassifier(),
        'params': {'max_depth': [3, 5], 'min_samples_split': [2, 5]}
    },
    'Random Forest': {
        'model': RandomForestClassifier(),
        'params': {'n_estimators': [50, 100], 'max_depth': [5, 10]}
    }
}

# Initialize results dictionary to store evaluation metrics
results = {}
best_tree_model = None

for model_name, config in param_grids.items():
    print(f"Tuning {model_name}...")
    
    # GridSearchCV setup and fitting
    grid_search = GridSearchCV(
        config['model'], 
        config['params'], 
        cv=5, 
        scoring='roc_auc'
    )
    grid_search.fit(X_train_resampled, y_train_resampled)
    
    # Save the best model
    best_model = grid_search.best_estimator_
    y_pred = best_model.predict(X_test)
    y_proba = best_model.predict_proba(X_test)[:, 1] if hasattr(best_model, "predict_proba") else None

    # Save the best Decision Tree for visualization
    if model_name == 'Decision Tree':
        best_tree_model = best_model

    # Calculate metrics
    metrics = {
        'Best Parameters': grid_search.best_params_,
        'CV AUC': grid_search.best_score_,
        'Accuracy': accuracy_score(y_test, y_pred),
        'Precision': precision_score(y_test, y_pred, pos_label=1),
        'Recall': recall_score(y_test, y_pred, pos_label=1),
        'F1 Score': f1_score(y_test, y_pred, pos_label=1),
    }
    if y_proba is not None:
        metrics['ROC AUC'] = roc_auc_score(y_test, y_proba)
    
    results[model_name] = metrics

# Display results for each model
print("Model Performance Metrics:\n")
for model_name, metrics in results.items():
    print(f"Model: {model_name}")
    for metric, value in metrics.items():
        print(f"  {metric}: {value}")
    print("\n")

# Compare key metrics like ROC AUC and F1 Score
best_model_name = max(results, key=lambda x: results[x]['ROC AUC'])
print(f"The best model based on ROC AUC is: {best_model_name}")

# Visualizing the Decision Tree
if 'best_tree_model' in locals() and best_tree_model is not None:
    print("Displaying the best Decision Tree model...")
    
    # Create the figure
    plt.figure(figsize=(20, 12))  # Increased size for better visualization
    
    # Plot the tree
    plot_tree(
        best_tree_model,
        feature_names=X.columns.tolist(),  # Ensure feature names are passed correctly
        class_names=["No Default", "Default"],  # Provide clear class names
        filled=True,  # Color-coded nodes
        rounded=True  # Rounded edges for improved readability
    )
    
    # Title and formatting
    plt.title("Decision Tree Visualization", fontsize=16)
    plt.axis("off")  # Remove axis for a cleaner view
    plt.show()
else:
    print("Error: Best Decision Tree model is not defined or available.")

# Evaluate the Decision Tree
y_pred = best_tree_model.predict(X_test)
y_proba = best_tree_model.predict_proba(X_test)[:, 1]

# Print the classification report
print("Classification Report for Decision Tree:")
print(classification_report(y_test, y_pred))

# Calculate and print ROC AUC score
roc_auc = roc_auc_score(y_test, y_proba)
print(f"ROC AUC Score for Decision Tree: {roc_auc}")